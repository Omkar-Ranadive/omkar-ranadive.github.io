---
title: "Variational Lower Bounds (ELBO) "
permalink: /posts/stats-ELBO
quora: false 
code: false 
study: true
category: statistics
tags:
  - statistics
---
**Original Video link:**
* [Variational Lower Bounds](https://youtu.be/pStDscJh2Wo)

**Credits: All images used in this post are courtesy of [Hugo Larochelle](https://www.youtube.com/channel/UCiDouKcxRmAdc5OeZdiRwAg)** 
<hr>

The idea of putting a lower bound on log of a function is based on idea
of log concavity:

![](../../images/stats/ELBO/media/image1.png)

As logarithmic function is concave, it is true that log(sum(wi\*ai)) \>=
sum(wi\*log(ai)).

We exploit this idea to have lower bounds on our likelihood functions.

![](../../images/stats/ELBO/media/image2.png)

Here h<sup>(1)</sup> is the latent variable; i.e., we say that
probability of our data x is based on latent variables h (variables
which we cannot directly observe but we can use them to inference about
our data).

As we can see, we have simply applied the log concavity idea to get the
\>= shown above.

**Also, note that here the distribution q(h|x) is any arbitrary
distribution. The choice of this distribution is up to us.**

![](../../images/stats/ELBO/media/image3.png)

We can see an interesting property. If the chosen distribution q(h|x) is
same as p(h|x) then the right hand side equation reduces to log(p(x));
hence we are just calculating the likelihood directly.

**Now, we know p(x, h) = p(x | h)\*p(h), hence, log(p(x, h)) will be
log(p(x|h))\*p(h).**

We use this idea as follows:

![](../../images/stats/ELBO/media/image4.png)

Then, we get the following:

![](../../images/stats/ELBO/media/image5.png)

Which can be refactored as follows:

![](../../images/stats/ELBO/media/image6.png)

![](../../images/stats/ELBO/media/image7.png)

**Note:** The negative sign in front of KL divergence is because we need
to invert log(p(x)/q(x)) to

\-log(q(x)/p(x)) to bring it in KL divergence form.
